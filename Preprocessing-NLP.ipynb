{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b29b8a2",
   "metadata": {},
   "source": [
    "# Data Preprocessing Techniques for Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc09797",
   "metadata": {},
   "source": [
    "## 1. Bag of Words\n",
    "\n",
    "The idea is to **represent each sentence as a bag of words**, disregarding grammar and paradigms, i.e., just the occurrence of words in a sentence defines the meaning of the sentence for the model.\n",
    "\n",
    "For example: Given two sentence \"I have a dog\" and \"You have a cat\", the first sentence (“I have a dog”) representation becomes 1,1,1,1,0,0, while the second sentence (“You have a cat”) representation becomes 0,1,1,0,1,1.\n",
    "\n",
    "Pros and Cons of Bag of Words:\n",
    "\n",
    "1. Pros: \n",
    "    1. Simple to implement, easy to understand.\n",
    "2. Cons:\n",
    "    1. If our input data is big, that would mean that the vocabulary size will also increase. This, in turn, makes our representation matrix much larger and makes computations very complex.\n",
    "    2. Computational nightmare is the inclusion of many 0s in our matrix (i.e., a sparse matrix). A sparse matrix contains less information and wastes a lot of memory.\n",
    "    3. The biggest disadvantage in Bag-of-Words is the complete inability to learn grammar and semantics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c13e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bag of words\n",
    "def calculate_bag_of_words(text, sentence):\n",
    "    # create a dictionary for frequency check\n",
    "    freqDict = dict.fromkeys(text, 0)\n",
    "    # loop over the words in sentences\n",
    "    for word in sentence:\n",
    "        # update word frequency\n",
    "        freqDict[word]=sentence.count(word)\n",
    "    # return dictionary \n",
    "    return freqDict\n",
    "\n",
    "text = ['I', 'have', 'a', 'dog', 'you', 'cat']\n",
    "s1 = \"I have a dog\"\n",
    "s2 = \"You have a cat\"\n",
    "ans1 = calculate_bag_of_words(text, s1)\n",
    "ans2 = calculate_bag_of_words(text, s2)\n",
    "print(ans1)\n",
    "print(ans2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41381a93",
   "metadata": {},
   "source": [
    "## 2. Word2Vec\n",
    "Word2Vec essentially means expressing each word in your text corpus in an N-dimensional space (embedding space).  We help define the meaning of words based on their context.\n",
    "\n",
    "There are two subsets of Word2Vec:\n",
    "1. Countinous Bag-of-Words (CBOW)\n",
    "2. SkipGram\n",
    "\n",
    "### CBOW\n",
    "CBOW is a technique where, given the neighboring words, the center word is determined. If our input sentence is **“I am reading the book.”**, then the input pairs and labels for a window size of 3 would be:\n",
    "- I, reading, for the label am\n",
    "- am, the, for the label reading\n",
    "- reading, book, for the label \n",
    "\n",
    "### Skip-Gram\n",
    "Skip-Gram approach is given the center word, we have to predict its neighboring words. Quite literally the opposite of CBOW, but more efficient. Before we get to that, let’s understand what Skip-Gram is.\n",
    "\n",
    "Let our given input sentence be “I am reading the book.” The corresponding Skip-Gram pairs for a window size of 3 would be:\n",
    "\n",
    "- am, for labels I and reading\n",
    "- reading, for labels am and the\n",
    "- the, for labels reading and "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59532a40",
   "metadata": {},
   "source": [
    "## 3. GLOVE\n",
    "\n",
    "Glove is based on **matrix factorization techniques** on the word-context matrix. It first constructs a large matrix of (words x context) co-occurrence information, i.e. for each “word” (the rows), you count how frequently we see this word in some “context” (the columns) in a large corpus.  The number of “contexts” is of course large, since it is essentially combinatorial in size.\n",
    "\n",
    "What is the different between Word2Vec and GLOVE?\n",
    "\n",
    "    Word2vec embeddings are based on training a shallow feedforward neural network while glove embeddings are learnt based on matrix factorization techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f89cd1",
   "metadata": {},
   "source": [
    "## Reference\n",
    "- [Word2Vec: A Study of Embeddings in NLP](https://pyimagesearch.com/2022/07/11/word2vec-a-study-of-embeddings-in-nlp/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
