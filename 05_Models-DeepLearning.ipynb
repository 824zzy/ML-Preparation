{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a7adae6",
   "metadata": {},
   "source": [
    "# Deep Learning Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0eb0793e",
   "metadata": {},
   "source": [
    "## 1. Siamese Network\n",
    "\n",
    "- Siamese networks are a type of neural network architecture that contain two or more identical subnetworks. \n",
    "- The **subnetworks share the same weights** and learn to solve the same problem, **but with different inputs**. \n",
    "- The subnetworks are trained to **minimize the difference between their outputs on pairs of inputs that are similar, and maximize the difference between their outputs on pairs of inputs that are dissimilar**.\n",
    "- SentenceBert is a variant of Siamese Network."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1401b440",
   "metadata": {},
   "source": [
    "## 2. xDeepFM\n",
    "\n",
    "1. xDeepFM is a deep learning model that combines the benefits of **wide and deep learning models**.\n",
    "2. There are three main components of xDeepFM:\n",
    "    1. **Wide component**: This component is a linear model(Embedding Layer) that captures the **implicit feature interactions**.\n",
    "    2. **Deep component**: This component is a deep neural network(Plain DNN) that captures the **feature representations**.\n",
    "    3. **Cross component**: This component is a Compression Interaction Network(CIN) that captures the **explicit feature interactions**.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./images/2023-02-03.jpeg\" width=50% height=50%>\n",
    "</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "43923f96",
   "metadata": {},
   "source": [
    "## 3. Long Short-Term Memory (LSTM)\n",
    "\n",
    "LSTM (Long Short-Term Memory) is **a type of Recurrent Neural Network (RNN)** that is widely used in Natural Language Processing (NLP) and speech recognition tasks. It is designed to **handle the issue of vanishing gradients** in traditional RNNs by introducing a memory cell, gates (input, forget, output) and cell state that control the flow of information into and out of the memory cell. Due to the additive update mechanism, the LSTM's memory cell ensures gradients remain consistent over lengthy sequences.\n",
    "\n",
    "The basic idea behind LSTMs is to allow information to persist in the network for a longer time period. The input gate regulates the flow of input data into the memory cell. The forget gate decides what information should be discarded from the memory cell. The output gate controls the flow of information from the memory cell to the output. The cell state acts as a “memory” that retains information for an extended period of time.\n",
    "\n",
    "The LSTM network takes **the current input, previous hidden state, and previous cell state as input** and **produces the current hidden state and cell state**, which are then used to make predictions. The hidden state and cell state are then passed as input to the next time step in a sequence.\n",
    "\n",
    "Overall, LSTMs help in modeling the dependencies between elements in a sequence, making it a powerful tool in sequential data modeling."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b4b2fdff",
   "metadata": {},
   "source": [
    "## 4. Transformer\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./images/2022-09-08-11-01-25.png\" width=50% height=50%>\n",
    "</p>\n",
    "\n",
    "In natural language processing (NLP), the Transformer is a neural network architecture that was introduced in a 2017 paper by Google researchers. It is used for tasks such as language translation, text summarization, and question answering.\n",
    "\n",
    "The Transformer architecture is based on the idea of self-attention, which allows the model to weigh the importance of different parts of the input when making predictions. It uses multi-head self-attention mechanism, which means it uses multiple attention heads to weigh the importance of different parts of the input. This allows the model to better understand the relationships between words in a sentence, which helps it generate more accurate and coherent text.\n",
    "\n",
    "The Transformer architecture also uses a technique called positional encoding, which allows the model to understand the order of words in a sentence. Additionally, it uses a feed-forward neural network to further process the output of the self-attention mechanism.\n",
    "\n",
    "The transformer architecture has become popular in many NLP tasks and its variants such as BERT, GPT, GPT-2, GPT-3 etc have been used extensively in many NLP tasks and have provided state of the art results.\n",
    "\n",
    "## Formula of Attention\n",
    "\n",
    "$$ Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V $$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0114db0b",
   "metadata": {},
   "source": [
    "## 5. Pre-training\n",
    "\n",
    "### Masked Language Model (MLM)\n",
    "\n",
    "MLM is a pre-training technique used in NLP. It is used to train a language model by masking some of the words in a sentence and then training the model to predict the masked words.\n",
    "\n",
    "### Causal Language Model (CLM)\n",
    "\n",
    "CLM predicts the token after a sequence of tokens."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0e051b85",
   "metadata": {},
   "source": [
    "## 6. BERT\n",
    "\n",
    "BERT, which stands for **Bidirectional Encoder Representations from Transformers**, is based on Transformers and was introduced in a 2018 paper by Google researchers. It is used for tasks such as question answering, text classification, and named entity recognition.\n",
    "\n",
    "The BERT framework was pre-trained using text from Wikipedia and can be fine-tuned with question and answer datasets, such as SQuAD, to generate answers to questions. It can also be fine-tuned with text classification datasets, such as the GLUE benchmark, to classify text into different categories.\n",
    "\n",
    "BERT Base Model has 12 Layers and 110M parameters with 768 Hidden and equal embedding layers. This large size makes it very computationally heavy to train.\n",
    "\n",
    "### BERT Input\n",
    "\n",
    "1. BERT takes two inputs: the tokenized sentence and the segment IDs. The tokenized sentence is a list of integers, where each integer represents a token in the sentence. The segment IDs are also a list of integers, where each integer represents the segment to which the corresponding token belongs. The segment IDs are used to differentiate between the two sentences in a pair of sentences. The segment IDs are always 0 for single sentences, and 0 and 1 for pairs of sentences.\n",
    "\n",
    "### BERT Family\n",
    "\n",
    "1. ALBERT: A Lite BERT has 12 million parameters with 768 hidden layers and 128 embedding layers, the following 2 techniques are used\n",
    "   1. Cross-layer parameter sharing: In this method, the parameter of only the first encoder is learnt and the same is used across all encoders.\n",
    "   2. Factorized embedding layer parameterization: Instead of keeping the embedding layer at 768, the embedding layer is reduced by factorization to 128 layers.\n",
    "2. RoBERTa: RoBERTa stands for “Robustly Optimized BERT pre-training Approach”. In many ways this is a better version of the BERT model. The key points of difference are as follows:\n",
    "   1. Dynamic Masking: BERT uses static masking i.e. the same part of the sentence is masked in each Epoch. In contrast, RoBERTa uses dynamic masking, wherein for different Epochs different part of the sentences are masked. This makes the model more robust.\n",
    "   2. Remove NSP Task: It was observed that the NSP task is not very useful for pre-training the BERT model. Therefore, the RoBERTa only with the MLM task.\n",
    "   3. More data Points: BERT is pre-trained on “Toronto BookCorpus” and “English Wikipedia datasets” i.e. as a total of 16 GB of data. In contrast, in addition to these two datasets, RoBERTa was also trained on other datasets like CC-News (Common Crawl-News), Open WebText etc. The total size of these datasets is around 160 GB.\n",
    "   4. Large Batch size: To improve on the speed and performance of the model, RoBERTa used a batch size of 8,000 with 300,000 steps. In comparison, BERT uses a batch size of 256 with 1 million steps.\n",
    "3. ELECTRA: ELECTRA stands for “Efficiently Learning an Encoder that Classifies Token Replacements Accurately”. The model uses a generator-discriminator structure. Other than being the lighter version of BERT, ELECTRA has the following distinguishing features:\n",
    "   1. Replaced Token Detection: Instead of MLM for pre-training, ELECTRA uses a task called “Replaced Token Detection” (RTD). In RTD, instead of masking the token, the token is replaced by a wrong token and the model is expected to classify, whether the tokens are replaced with wrong or not.\n",
    "   2. No NSP pre-training is performed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4adf0b2",
   "metadata": {},
   "source": [
    "## 7. T5\n",
    "\n",
    "T5 is a text-to-text transformer model that was trained on a large corpus of text-to-text data. T5 is a successor to BERT and GPT-2, and is the first text-to-text transformer model that is trained on a large scale with a denoising objective. The changes compared to BERT include:\n",
    "\n",
    "1. Adding a causal decoder to the bidirectional architecture.\n",
    "2. Replacing the fill-in-the-blank cloze task with a mix of alternative pre-training tasks.\n",
    "\n",
    "![T5 model structure](./images/2022-09-08-11-11-34.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4989b2aa",
   "metadata": {},
   "source": [
    "## 8. ChatGPT\n",
    "\n",
    "ChatGPT is a variant of the GPT (Generative Pre-trained Transformer) model, which is a type of language model that uses deep learning to generate human-like text. The model is trained on a large dataset of text and learns patterns in the data to generate new text that is similar to the input it was trained on.\n",
    "\n",
    "When given a prompt or input, ChatGPT uses the patterns it learned during training to generate a response. The model uses a technique called \"auto-regression,\" where it predicts the next word in a sentence based on the previous words. This allows it to generate text that is contextually appropriate and coherent.\n",
    "\n",
    "It uses transformer architecture and pre-training the model on a large dataset of text, then fine-tuning it on a smaller dataset specific to the task at hand. The pre-training allows the model to learn general patterns in language, while fine-tuning allows it to focus on the specific task and generate more accurate and relevant responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91572e4a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "38cca0c38332a56087b24af0bc80247f4fced29cb4f7f437d91dc159adec9c4e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
