{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3da69403",
   "metadata": {},
   "source": [
    "# Activation Functions\n",
    "\n",
    "Activation functions are of two types based on how it is used in an ML model.\n",
    "\n",
    "1. Activation functions that are used in **output layers** of ML models. The primary purpose of these activation functions is to **squash the value between a bounded range like 0 to 1**.\n",
    "   1. Sigmoid: $f(x) = \\frac{1}{1+e^{-x}}$\n",
    "   2. Softmax: $f(x) = \\frac{e^x}{\\sum_{i=1}^{n}e^x}$\n",
    "   3. Tanh: $f(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$\n",
    "2. Activation functions that are used in **hidden layers** of neural networks. The primary purpose of these activation functions is to **provide non-linearity without which neural networks cannot model non-linear relationships**. This type of activation function should ideally satisfy the following conditions: **Non-linear** to let neural network learn non-linear relationships, **Unbounded** to enable faster learning and avoid saturating early, **Continuously differentiable**.\n",
    "   1. ReLU: $f(x) = max(0, x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01dd477",
   "metadata": {},
   "source": [
    "## 1. RELU\n",
    "\n",
    "What are the advantages of ReLU over sigmoid function in deep neural networks?\n",
    "\n",
    "1. **Reduced likelihood of vanishing gradient**: $h$ arises when $a$>0. In this regime the gradient has a constant value. In contrast, the gradient of sigmoids becomes increasingly small as the absolute value of x increases. The constant gradient of ReLUs results in faster learning.\n",
    "2. **Sparsity**: Sparsity arises when $a$â‰¤0. The more such units that exist in a layer the more sparse the resulting representation.\n",
    "3. **Better convergence performance**\n",
    "4. **More computationally efficient**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9563a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1498, 0.4028, 2.1596, 1.0192, 0.9494])\n",
      "tensor([0.1498, 0.4028, 2.1596, 1.0192, 0.9494])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "relu = torch.nn.ReLU()\n",
    "A = torch.randn(5)\n",
    "print(A)\n",
    "ans = relu(A)\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2693abf8",
   "metadata": {},
   "source": [
    "## 2. Sigmoid and Softmax\n",
    "\n",
    "Sigmoid formula: $f(x) = \\frac{1}{1+e^{-x}}$, and its derivative: $f'(x) = f(x)(1-f(x))$\n",
    "\n",
    "Softmax formula: $f(x) = \\frac{e^x}{\\sum_{i=1}^{n}e^x}$\n",
    "\n",
    "What is the difference between sigmoid and softmax functions?\n",
    "\n",
    "1. Sigmoid function is used in the output layer of a binary classification model. It squashes the output between 0 and 1. The output of sigmoid function is interpreted as the probability of the input belonging to class 1.\n",
    "2. Softmax function is used in the output layer of a multi-class classification model. It squashes the output between 0 and 1. The output of softmax function is interpreted as the probability of the input belonging to each class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64498ba5",
   "metadata": {},
   "source": [
    "## 3. Tanh\n",
    "Tanh formula: $f(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2833e8d8",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- [https://stats.stackexchange.com/questions/126238/what-are-the-advantages-of-relu-over-sigmoid-function-in-deep-neural-networks](https://stats.stackexchange.com/questions/126238/what-are-the-advantages-of-relu-over-sigmoid-function-in-deep-neural-networks)\n",
    "- [https://towardsdatascience.com/fantastic-activation-functions-and-when-to-use-them-481fe2bb2bde](https://towardsdatascience.com/fantastic-activation-functions-and-when-to-use-them-481fe2bb2bde)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb973f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('py310')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4 (main, Mar 31 2022, 03:38:35) [Clang 12.0.0 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "d1a940d2a5a085e4f840c6fa90dace1e4a81d9a7fba180f0fcbb4947149ff9d2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
