{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "586082f1",
   "metadata": {},
   "source": [
    "# Metrics for Evaluating Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a273f6",
   "metadata": {},
   "source": [
    "## Distance Metrics\n",
    "\n",
    "1. Euclidean Distance\n",
    "\n",
    "![ed](2022-09-07-12-17-59.png)\n",
    "\n",
    "2. Manhattan Distance\n",
    "\n",
    "![MD](2022-09-07-12-18-24.png)\n",
    "\n",
    "3. Minkowski Distance\n",
    "\n",
    "![MD](2022-09-07-12-18-48.png)\n",
    "\n",
    "4. Hamming Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1a4fde",
   "metadata": {},
   "source": [
    "## Evaluation Metrics\n",
    "\n",
    "### Precision and Recall\n",
    "\n",
    "Precision and recall are two metrics that are used to evaluate the performance of a classification model. They are defined as follows:\n",
    "\n",
    "- **Precision**: The precision of a model is the ratio of the number of true positives to the total number of true positives and false positives. It is a measure of the model’s accuracy. A model with high precision is more likely to predict a positive class when it is actually positive.\n",
    "- **Recall**: The recall of a model is the ratio of the number of true positives to the total number of true positives and false negatives. It is a measure of the model’s completeness. A model with high recall is more likely to predict a positive class when it is actually negative.\n",
    "\n",
    "![PR](2022-09-07-12-08-30.png)\n",
    "\n",
    "### F1 score\n",
    "\n",
    "The F1-score of a classification model is calculated as the harmonic mean of the precision and recall of the model. \n",
    "\n",
    "It is a good measure to use if you have an uneven class distribution (i.e. a lot more positive samples than negative samples).\n",
    "\n",
    "![f1](2022-09-07-12-06-25.png)\n",
    "\n",
    "### ROC and AUC\n",
    "\n",
    "The ROC curve is a plot of the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings. The AUC is the area under the ROC curve.\n",
    "\n",
    "AUC is the larger the better. AUC is useful as a single number summary of classifier performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bff319e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
