{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20815588",
   "metadata": {},
   "source": [
    "# Math and Concepts in Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa26822c",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Differentiate between correlation and covariance\n",
    "\n",
    "**Covariance shows you how the two variables differ, whereas correlation shows you how the two variables are related.**\n",
    "\n",
    "- Covariance is a statistical term that refers to a systematic relationship between two random variables in which a change in the other reflects a change in one variable.\n",
    "\n",
    "$${\\displaystyle \\operatorname {cov} (X,Y)=\\operatorname {E} \\left[(X-\\mu _{X})(Y-\\mu _{Y})\\right]}$$\n",
    "\n",
    "- Correlation is a measure that determines the degree to which two or more random variables move in sequence.\n",
    "\n",
    "$$\\rho _{XY}={\\frac {\\operatorname {cov} (X,Y)}{\\sigma _{X}\\sigma _{Y}}}$$\n",
    "\n",
    "-----\n",
    "### Combat the curse of dimensionality\n",
    "\n",
    "1. Manual Feature Selection\n",
    "2. Principal Component Analysis (PCA)\n",
    "3. Multidimensional Scaling\n",
    "4. Locally linear embedding\n",
    "\n",
    "-----\n",
    "### What is Momentum (w.r.t NN optimization)?\n",
    "\n",
    "Momentum lets the optimization algorithm remembers its last step, and adds some proportion of it to the current step. This way, even if the algorithm is stuck in a flat region, or a small local minimum, it can get out and continue towards the true minimum.\n",
    "\n",
    "-----\n",
    "### What is the difference between Batch Gradient Descent and Stochastic Gradient Descent?\n",
    "\n",
    "Batch gradient descent computes the gradient using the whole dataset. This is great for convex, or relatively smooth error manifolds. In this case, we move somewhat directly towards an optimum solution, either local or global. Additionally, batch gradient descent, given an annealed learning rate, will eventually find the minimum located in it's basin of attraction.\n",
    "\n",
    "Stochastic gradient descent (SGD) computes the gradient using a single sample. SGD works well (Not well, I suppose, but better than batch gradient descent) for error manifolds that have lots of local maxima/minima. In this case, the somewhat noisier gradient calculated using the reduced number of samples tends to jerk the model out of local minima into a region that hopefully is more optimal.\n",
    "\n",
    "-----\n",
    "### What is vanishing gradient?\n",
    "\n",
    "As we add more and more hidden layers, back propagation becomes less and less useful in passing information to the lower layers. In effect, as information is passed back, the gradients begin to vanish and become small relative to the weights of the networks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b5e9eb",
   "metadata": {},
   "source": [
    "## Engineering\n",
    "\n",
    "### What's the benefit of using Numpy?\n",
    "1. Efficient Array Operations.\n",
    "2. Broadcasting to perform operations on arrays of different shapes.\n",
    "3. Mathematical Functions including linear algebra, Fourier transform, and random number generation.\n",
    "4. Memory-efficient container that provides fast numerical operations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4bfdba",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
