{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "586082f1",
   "metadata": {},
   "source": [
    "# Metrics for Evaluating Performance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "17a273f6",
   "metadata": {},
   "source": [
    "-----\n",
    "### The trade-off between bias and variance\n",
    "\n",
    "Bias: how well a model fits data. \n",
    "    \n",
    "    Model with high bias pays very little attention to the training data and oversimplifies the model. It always leads to **high error** on training and test data.\n",
    "\n",
    "Variance: how much a model changes based on inputs\n",
    "\n",
    "    Model with high variance pays a lot of attention to training data and **does not generalize** on the data which it hasn’t seen before.\n",
    "\n",
    "- Low variance and high bias ==> underfitting (model too simple)\n",
    "- High variance and low bias ==> overfitting (model too complex)\n",
    "- High variance and high bias ==> garbage\n",
    "- Low variance and low bias ==> perfect\n",
    "\n",
    "-----\n",
    "\n",
    "### What is overfitting and how to avoid overfitting?\n",
    "1. Regularization. It involves a cost term for the features involved with the objective function\n",
    "2. Making a simple model. With lesser variables and parameters, the variance can be reduced \n",
    "3. Cross-validation methods like k-folds can also be used\n",
    "4. If some model parameters are likely to cause overfitting, techniques for regularization like LASSO can be used that penalize these parameters\n",
    "\n",
    "-----\n",
    "## Distance Metrics\n",
    "\n",
    "1. Manhattan Distance: $\\sum_{i=1}^{n}|x_i - y_i|$\n",
    "2. Euclidean Distance: $\\sqrt{\\sum_{i=1}^{n}(x_i - y_i)^2}$\n",
    "3. Minkowski Distance: $\\sqrt[p]{\\sum_{i=1}^{n}|x_i - y_i|^p}$\n",
    "4. Hamming Distance: $\\sum_{i=1}^{n}I(x_i \\neq y_i)$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cd1a4fde",
   "metadata": {},
   "source": [
    "## Classification Evaluation Metrics\n",
    "\n",
    "- **Accuracy**: The accuracy of a model is the ratio of the number of correct predictions to the total number of predictions. It is a measure of the model’s correctness. \n",
    "  - When to use accuracy? When the classes are balanced, say, **we can not use accuracy to predict if an asteroid is going to hit the earth or not.**\n",
    "\n",
    "$$\\text{Accuracy} = \\frac{\\text{True Positives} + \\text{True Negatives}}{\\text{True Positives} + \\text{True Negatives} + \\text{False Positives} + \\text{False Negatives}}$$\n",
    "\n",
    "- **Precision**: The precision of a model is the ratio of the number of true positives to all the predicted positives. A model with high precision is more likely to predict a positive class when it is actually positive.\n",
    "  - When to use precision? When the classes are imbalanced and we want to be very sure of our prediction, say, **we can use precision to predict if an email is spam or not.**\n",
    "\n",
    "$$\\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}}$$\n",
    "\n",
    "- **Recall**: The recall of a model is the ratio of the number of true positives to the total number of true positives and false negatives. It is a measure of the proportion of actual positives are correctly classified. A model with high recall is more likely to predict a positive class when it is actually negative.\n",
    "  - When to use recall? When we want to predict all the positive classes, say, **we can use recall to predict if a patient has a disease or not.**\n",
    "\n",
    "$$\\text{Recall} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}}$$\n",
    "\n",
    "- **F1 score**: The F1-score of a classification model is calculated as the harmonic mean of the precision and recall of the model.  It is a good measure to use if you have an uneven class distribution (i.e. a lot more positive samples than negative samples). It is maximum when Precision is equal to Recall.\n",
    "  - When to use F1 score? When False Positive and False Negative are equally costly and True Negative is high\n",
    "\n",
    "$$\\text{F1} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}$$\n",
    "\n",
    "- **ROC and AUC**\n",
    "  - The ROC curve is a plot of the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings. \n",
    "  - The AUC is the area under the ROC curve.\n",
    "  - AUC is the larger the better. AUC is useful as a single number summary of classifier performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e764b7b2",
   "metadata": {},
   "source": [
    "# Confusion Matrix\n",
    "\n",
    "Definition: A confusion matrix is a table that is often used to describe the performance of a classification model (or “classifier”) on a set of test data for which the true values are known.\n",
    "\n",
    "- **True Positive (TP)**: The number of correct predictions that an instance is positive.\n",
    "- **True Negative (TN)**: The number of correct predictions that an instance is negative.\n",
    "- **False Positive (FP)**: The number of incorrect predictions that an instance is positive.\n",
    "- **False Negative (FN)**: The number of incorrect predictions that an instance is negative."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6f9452fc",
   "metadata": {},
   "source": [
    "## Natural Language Generation Evaluation Metrics\n",
    "\n",
    "- **BLEU**: BLEU is a metric for evaluating a generated sentence to a reference sentence. It is calculated by **comparing n-grams of the generated sentence to the n-grams of the reference sentence**. The higher the BLEU score, the better the generated sentence is.\n",
    "  - $\\text{BLEU} = \\exp\\left(\\frac{1}{N} \\sum_{n=1}^{N} \\log p_n\\right)$, where $p_n$ is the precision of $n$-grams.\n",
    "  - Disadvantage of BLUE: It is **overdependent on reference** and it not a good metric for evaluating the **fluency/grammar** of the generated sentence.\n",
    "- METEOR:\n",
    "- CIDEr:\n",
    "- ChrF++:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbabf7f9",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
