{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d546828",
   "metadata": {},
   "source": [
    "# Data Preprocessing Techniques for Natural Language Processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c965c64d",
   "metadata": {},
   "source": [
    "-----\n",
    "## Deal with imbalanced data\n",
    "\n",
    "1. Resampling techniques for data preprocessing\n",
    "   1. Oversampling: Increase the number of samples in the minority class by randomly duplicating existing samples or generating synthetic samples using techniques like SMOTE (Synthetic Minority Over-sampling Technique).\n",
    "   2. Undersampling: Reduce the number of samples in the majority class by randomly removing instances.\n",
    "2. Algorithmic Techniques\n",
    "   1. Class Weights: assign different weights to classes, i.e., increasing the weight of the minority class can help the model pay more attention to it during training.\n",
    "   2. Ensemble Methods: Techniques like bagging and boosting can be effective in handling class imbalance. Algorithms like Random Forest and AdaBoost can automatically handle imbalanced datasets by adjusting their training process\n",
    "3. Use appropriate loss functions. We can use loss functions that are less sensitive to class imbalance, such as the weighted cross-entropy loss.\n",
    "4. Use appropriate metrics. We can use metrics that are less sensitive to class imbalance, such as the F1 score, AUC-ROC, and AUC-PR.\n",
    "\n",
    "-----\n",
    "## Handle missing or corrupted data in a dataset\n",
    "\n",
    "- Dropping the rows or columns with the missing or corrupted dataset\n",
    "- replacing them entirely with a different value are two easy ways to handle such a situation.\n",
    "\n",
    "Methods like IsNull(), dropna(), and Fillna() help in accomplishing this task.\n",
    "\n",
    "-----\n",
    "## Normalization\n",
    "\n",
    "Normalization transforms features to be on a similar scale.\n",
    "\n",
    "$$X_{new} = \\frac{X - X_{min}}{X_{max} - X_{min}}$$\n",
    "\n",
    "-----\n",
    "## Standardization\n",
    "\n",
    "Standardization is a technique to transform the data into a standard normal distribution with mean 0 and standard deviation\n",
    "\n",
    "It is also called as Z-score normalization. It is done by subtracting the mean and dividing by the standard deviation of each value.\n",
    "\n",
    "$$X_{new} = \\frac{X - \\mu}{\\sigma}$$\n",
    "\n",
    "-----\n",
    "## TF-IDF\n",
    "\n",
    "TF-IDF formula: $tfidf(t,d) = tf(t,d) * idf(t)$\n",
    "\n",
    "TF-IDF stands for **Term Frequency-Inverse Document Frequency**. It is a **numerical statistic** that is intended to reflect how important a word is to a document in a collection or corpus. It is often used as a weighting factor in information retrieval and text mining.\n",
    "\n",
    "- Term frequency, tf(t,d): **relative frequency of term t within document d**.\n",
    "- The inverse document frequency: **logarithmically scaled inverse fraction of the documents that contain the word**.\n",
    "\n",
    "## Cross-Validation\n",
    "\n",
    "Cross-validation is a method of splitting all your data into three parts: training, testing, and validation data. Data is split into k subsets, and the model has trained on k-1of those datasets. \n",
    "\n",
    "The last subset is held for testing. This is done for each of the subsets. This is k-fold cross-validation.\n",
    "\n",
    "Finally, the scores from all the k-folds are averaged to produce the final score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e671d872",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
