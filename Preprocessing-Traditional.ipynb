{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d546828",
   "metadata": {},
   "source": [
    "# Data Preprocessing Techniques for Natural Language Processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c965c64d",
   "metadata": {},
   "source": [
    "## 1. Normalization\n",
    "\n",
    "Normalization ($X_new = (X - X_min)/(X_max - X_min)$) transforms features to be on a similar scale.\n",
    "\n",
    "## 2. Standardization\n",
    "\n",
    "Standardization ($X_{new} = (X - mean)/Std$) is a technique to transform the data into a standard normal distribution with mean 0 and standard deviation\n",
    "\n",
    "It is also called as Z-score normalization. It is done by subtracting the mean and dividing by the standard deviation of each value.\n",
    "\n",
    "## 3. Handle missing or corrupted data in a dataset\n",
    "\n",
    "- Dropping the rows or columns with the missing or corrupted dataset\n",
    "- replacing them entirely with a different value are two easy ways to handle such a situation.\n",
    "\n",
    "Methods like IsNull(), dropna(), and Fillna() help in accomplishing this task.\n",
    "\n",
    "## 4. TF-IDF\n",
    "\n",
    "TF-IDF formula: $tfidf(t,d) = tf(t,d) * idf(t)$\n",
    "\n",
    "TF-IDF stands for **Term Frequency-Inverse Document Frequency**. It is a **numerical statistic** that is intended to reflect how important a word is to a document in a collection or corpus. It is often used as a weighting factor in information retrieval and text mining.\n",
    "\n",
    "- Term frequency, tf(t,d), is the relative frequency of term t within document d.\n",
    "- The inverse document frequency is the logarithmically scaled inverse fraction of the documents that contain the word.\n",
    "\n",
    "## 5. Deal with imbalanced data\n",
    "\n",
    "1. **Oversampling or undersampling**. Instead of sampling with a uniform distribution from the training dataset, we can use other distributions so the model sees a more balanced dataset.\n",
    "2. **Data augmentation**. We can add data in the less frequent categories by modifying existing data in a controlled way. In the example dataset, we could flip the images with illnesses, or add noise to copies of the images in such a way that the illness remains visible.\n",
    "3. **Using appropriate metrics**. We can use metrics that are less sensitive to class imbalance, such as the F1 score.\n",
    "4. **Using appropriate loss functions**. We can use loss functions that are less sensitive to class imbalance, such as the weighted cross-entropy loss.\n",
    "\n",
    "## 6. Cross-Validation\n",
    "\n",
    "Cross-validation is a method of splitting all your data into three parts: training, testing, and validation data. Data is split into k subsets, and the model has trained on k-1of those datasets. \n",
    "\n",
    "The last subset is held for testing. This is done for each of the subsets. This is k-fold cross-validation.\n",
    "\n",
    "Finally, the scores from all the k-folds are averaged to produce the final score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e671d872",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
