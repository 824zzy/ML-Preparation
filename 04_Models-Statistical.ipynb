{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4e8900b",
   "metadata": {},
   "source": [
    "# Machine Learning Models\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "df3c7961",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "### Supervised VS Unsupervised Learning\n",
    "\n",
    "1. Supervised Learning:  In a supervised learning task, the data sample would contain a target attribute $y$, also known as ground truth.\n",
    "2. Unsupervised learning: We do not have the ground truth in an unsupervised learning task\n",
    "   1. Clustering\n",
    "   2. Association Rule Mining\n",
    "   3. Auto-encoders\n",
    "   4. Anomaly Detection\n",
    "\n",
    "### Classification VS Regression\n",
    "\n",
    "1. Classification: The target attribute $y$ is a discrete variable, such as a class label.\n",
    "2. Regression: The target attribute $y$ is a continuous variable, such as a real number."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f5d6e35e",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "Linear regression is a linear model, i.e. a model that assumes a linear relationship between the input variables (x) and the single output variable (y).\n",
    "\n",
    "$$y = w_0 + w_1x_1 + w_2x_2 + ... + w_nx_n$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "183e5fe1",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "\n",
    "Bayes' theorem formula: $P(A|B) = P(B|A) * P(A) / P(B)$\n",
    "\n",
    "The Naive Bayes method is a supervised learning algorithm, it is naive since it makes assumptions by applying Bayes’ theorem that all attributes are independent of each other.\n",
    "\n",
    "1. Learn the prior probabilities of each class, that is, the probability that a new observation belongs to each class. $P(A)$, $P(B)$\n",
    "2. Learn the likelihoods of each feature for each class, that is, the probability that a new observation has a particular value for a feature given that it belongs to a particular class. $P(B|A)$\n",
    "3. Compute the posterior probability of each class for a new observation, that is, the probability that the new observation belongs to each class given the values of its features."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "84a80811",
   "metadata": {},
   "source": [
    "## Principal component analysis (PCA)\n",
    "\n",
    "Principal component analysis (PCA) is most commonly used for dimension reduction. \n",
    "\n",
    "1. Standardize the data\n",
    "2. Compute the covariance matrix\n",
    "3. Compute eigen vectors of the covariance matrix\n",
    "4. Compute the explained variance and select N components\n",
    "5. Transform Data using eigen vectors\n",
    "6. Invert PCA and Reconstruct original data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eefb2f58",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "Logistic regression is a classification model that in its basic form uses a **logistic function** to model a **binary dependent variable**. In regression analysis, logistic regression (or logit regression) is estimating the parameters of a logistic model (a form of binary regression).\n",
    "\n",
    "$$P(y=1|x) = \\frac{1}{1+e^{-(\\beta_0 + \\beta_1 x)}}$$\n",
    "\n",
    "Can logistic regression use for more than 2 classes?\n",
    "\n",
    "    No, by default logistic regression is a binary classifier, so it cannot be applied to more than 2 classes. However, it can be extended for solving multi-class classification problems (multinomial logistic regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5558883",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors(KNN)\n",
    "\n",
    "KNN is a non-parametric method used for classification and regression. In both cases, the input consists of the k closest training examples in the feature space.\n",
    "\n",
    "1. Step1: Choose K value\n",
    "2. Step2: For each data point in the data:\n",
    "   1. Find the Euclidean distance to all training data samples\n",
    "   2. Store the distances on an ordered list and sort it\n",
    "   3. Choose the top K entries from the sorted list\n",
    "3. Step3: Label the test point based on the majority of classes present in the selected points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03c7fba",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mrequest to http://localhost:8888/api/sessions?1674748755844 failed, reason: connect ETIMEDOUT 127.0.0.1:8888. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from math import sqrt\n",
    "\n",
    "def KNN(train, test_row, num_neighbors):\n",
    "    def distance(row1, row2):\n",
    "        dist = 0.0\n",
    "        for i in range(len(row1)-1):\n",
    "            dist += (row1[i] - row2[i])**2\n",
    "        return sqrt(dist)\n",
    "\n",
    "    # get neighbors\n",
    "    distances = []\n",
    "    for train_row in train:\n",
    "        dist = distance(test_row, train_row)\n",
    "        distances.append((train_row, dist))\n",
    "    distances.sort(key=lambda x: x[1])\n",
    "    neighbors = []\n",
    "    for i in range(num_neighbors):\n",
    "        neighbors.append(distances[i][0])\n",
    "    # make a classification prediction with neighbors\n",
    "    output_values = [row[-1] for row in neighbors]\n",
    "    prediction = max(set(output_values), key=output_values.count)\n",
    "    return prediction\n",
    "\n",
    "\n",
    "# Test distance function\n",
    "dataset = [\n",
    "    [2.7810836,2.550537003,0],\n",
    "\t[1.465489372,2.362125076,0],\n",
    "\t[3.396561688,4.400293529,0],\n",
    "\t[1.38807019,1.850220317,0],\n",
    "\t[3.06407232,3.005305973,0],\n",
    "\t[7.627531214,2.759262235,1],\n",
    "\t[5.332441248,2.088626775,1],\n",
    "\t[6.922596716,1.77106367,1],\n",
    "\t[8.675418651,-0.242068655,1],\n",
    "\t[7.673756466,3.508563011,1]]\n",
    "prediction = KNN(dataset, dataset[0], 3)\n",
    "print('Expected %d, Got %d.' % (dataset[0][-1], prediction))\n",
    "\n",
    "# reference: https://machinelearningmastery.com/tutorial-to-implement-k-nearest-neighbors-in-python-from-scratch/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8eef09",
   "metadata": {},
   "source": [
    "## K-Means Clustering\n",
    "\n",
    "1. Randomly assign each data point to a cluster\n",
    "2. Determine the cluster centroid coordinates\n",
    "3. Determine the distances of each data point to the centroids and re-assign each point to the closest cluster centroid based upon minimum distance\n",
    "4. Calculate cluster centroids again\n",
    "5. Repeat steps 4 and 5 until we reach global optima where no improvements are possible and no switching of data points from one cluster to other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83568527",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mrequest to http://localhost:8888/api/sessions?1674748755844 failed, reason: connect ETIMEDOUT 127.0.0.1:8888. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist \n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.decomposition import PCA\n",
    " \n",
    "#Function to implement steps given in previous section\n",
    "def K_means(x, k, no_of_iterations):\n",
    "    idx = np.random.choice(len(x), k)\n",
    "    # Randomly choosing Centroids \n",
    "    centroids = x[idx, :] #Step 1\n",
    "    # Finding the distance between centroids and all the data points\n",
    "    distances = cdist(x, centroids ,'euclidean') #Step 2\n",
    "    #Centroid with the minimum Distance\n",
    "    points = np.array([np.argmin(i) for i in distances]) #Step 3\n",
    "     \n",
    "    #Repeating the above steps for a defined number of iterations\n",
    "    #Step 4\n",
    "    for _ in range(no_of_iterations): \n",
    "        centroids = []\n",
    "        for idx in range(k):\n",
    "            #Updating Centroids by taking mean of Cluster it belongs to\n",
    "            temp_cent = x[points==idx].mean(axis=0) \n",
    "            centroids.append(temp_cent)\n",
    " \n",
    "        centroids = np.vstack(centroids) #Updated Centroids \n",
    "         \n",
    "        distances = cdist(x, centroids ,'euclidean')\n",
    "        points = np.array([np.argmin(i) for i in distances])\n",
    "         \n",
    "    return points \n",
    "\n",
    "#Load Data\n",
    "data = load_digits().data\n",
    "pca = PCA(2)\n",
    "  \n",
    "#Transform the data\n",
    "df = pca.fit_transform(data)\n",
    "label = K_means(df, 10,10)\n",
    "print(label)\n",
    "\n",
    "# reference: https://www.askpython.com/python/examples/k-means-clustering-from-scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9988c885",
   "metadata": {},
   "source": [
    "## Support Vector Machine (SVM)\n",
    "\n",
    "Support Vector Machine (SVM) is a supervised machine learning algorithm which can be used for both **classification or regression challenges**.\n",
    "\n",
    "A Support Vector Machine (SVM) is an algorithm that tries to **fit a line (or plane or hyperplane) between the different classes** that **maximizes the distance** from the line to the points of the classes.\n",
    "\n",
    "What are the different kernels in SVM?\n",
    "\n",
    "1. Linear kernel - used when data is linearly separable.\n",
    "2. Polynomial kernel - When you have discrete data that has no natural notion of smoothness.\n",
    "3. Radial basis kernel - Create a decision boundary able to do a much better job of separating two classes than the linear kernel.\n",
    "4. Sigmoid kernel - used as an activation function for neural networks."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "243ee07a",
   "metadata": {},
   "source": [
    "## Ensemble Learning\n",
    "\n",
    "Ensemble is a machine learning concept, the basic idea is to learn a set of classifiers (experts) and to allow them to vote.\n",
    "\n",
    "Bagging and Boosting are two types of Ensemble Learning.\n",
    "\n",
    "The main difference between these learning methods is the method of training. In bagging, data scientists improve the accuracy of weak learners by **training several of them at once on multiple datasets**. In contrast, **boosting trains weak learners one after another**.\n",
    "\n",
    "1. What is bagging and boosting?\n",
    "   1. Bagging is democracy based politics for less variance\n",
    "      1. Step 1: **Multiple subsets** are created from the original data set with equal tuples, selecting observations with replacement.\n",
    "      2. Step 2: A base model is created on each of these subsets.\n",
    "      3. Step 3: Each model is learned in parallel with each training set and independent of each other.\n",
    "      4. Step 4: The final predictions are determined by **combining the predictions from all the models**.\n",
    "   2. Bagging is elite based politics for less bias\n",
    "      1. Step 1: Initialize the dataset and assign equal weight to each of the data point.\n",
    "      2. Step 2: Provide this as input to the model and identify the wrongly classified data points.\n",
    "      3. Step 3: **Increase the weight of the wrongly classified data points and decrease the weights of correctly classified data points**. And then normalize the weights of all data points.\n",
    "      4. Step 4: Loop until got required results.\n",
    "\n",
    "2. What are the types of boosting?\n",
    "   1. Adaptive Boosting (AdaBoost) is a boosting algorithm that uses a **weighted majority algorithm** to combine several weak classifiers into a strong classifier.\n",
    "   2. Gradient Boosting is a boosting algorithm that uses a **gradient descent algorithm** to combine several weak classifiers into a strong classifier.\n",
    "   3. Extreme Gradient Boosting (XGBoost) improves gradient boosting for computational speed and scale in several ways."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5aeda152",
   "metadata": {},
   "source": [
    "## Decision Tree and Random Forest\n",
    "\n",
    "### Decision Tree\n",
    "\n",
    "Decision trees are data structures in machine learning that work by dividing the dataset into smaller and smaller subsets based on their features.\n",
    "\n",
    "Decision tree VS Neural Network:\n",
    "\n",
    "1. **Interpretability ==> Decision Tree**: Decision trees are highly interpretable, as the logic behind the predictions is easy to understand and trace. Neural networks, on the other hand, are often considered \"black boxes\" because their internal workings are difficult to interpret. If interpretability is an important consideration, a decision tree might be a better choice.\n",
    "3. **Non-linearity ==> Neural Network**: Decision trees are suitable for problems with simple, linear relationships between inputs and outputs, while neural networks are better suited for problems with complex, non-linear relationships. If the relationship between the input and output variables is non-linear, a neural network would be a better choice.\n",
    "4. **Huge Number of features ==> Neural Network**: Decision trees are prone to overfitting when the number of features is very large, whereas neural networks can handle large numbers of features, but they require a large amount of data to do so. If the number of features is very large, a neural network would be a better choice.\n",
    "5. **Speed ==> Decision Tree**: Decision trees are relatively fast to train and predict, while neural networks can be quite computationally expensive to train and predict. If speed is an important consideration, a decision tree might be a better choice.\n",
    "\n",
    "### Random Forest\n",
    "\n",
    "Like bagging and boosting, random forest works by combining a set of other tree models. Random forest builds a tree from a random sample of the columns in the test data.\n",
    "\n",
    "Here’s are the steps how a random forest creates the trees:\n",
    "\n",
    "- Take a sample size from the training data.\n",
    "- Begin with a single node.\n",
    "- Run the following algorithm, from the start node:\n",
    "    - If the number of observations is less than node size then stop.\n",
    "    - Select random variables.\n",
    "    - Find the variable that does the “best” job of splitting the observations.\n",
    "    - Split the observations into two nodes.\n",
    "    - Call step `a` on each of these nodes.\n",
    "\n",
    "\n",
    "\n",
    "What parameters are most important for tree-based learners?\n",
    "\n",
    "1. max_depth - This is the maximum depth per tree. This adds complexity but at the benefit of boosting performance.\n",
    "2. learning_rate - This determines step size at each iteration. A lower learning rate slows computation, but increases the chance of reaching a model closer to theoptimum.\n",
    "3. n_estimators - This refers to the number of trees in an ensemble, or the number of boosting rounds.\n",
    "4. subsample - This is the fraction of observations to be sampled for each tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ca6afc",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mrequest to http://localhost:8888/api/sessions?1674748755844 failed, reason: connect ETIMEDOUT 127.0.0.1:8888. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from itertools import permutations\n",
    "\n",
    "def random_forest(new_point, data):\n",
    "    data = pd.DataFrame(data)\n",
    "    features = data.drop('Target',axis=1).columns\n",
    "    all_permutations = permutations(features)\n",
    "    votes = []\n",
    "    for permutation in all_permutations:\n",
    "        X = data\n",
    "        for col, i in zip(features,new_point):\n",
    "            X = X[X[col] == i].drop(col,axis=1)\n",
    "        vote = X.mode().values[0]\n",
    "        if not pd.isna(vote):\n",
    "            votes.append(vote)\n",
    "    return int(pd.DataFrame(votes).mode().values[0])\n",
    "\n",
    "new_point = [1, 1, 0, 0]\n",
    "data = {'Var1': {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 1.0, 9: 0.0, 10: 0.0, 11: 1.0, 12: 0.0, 13: 0.0, 14: 1.0, 15: 1.0, 16: 1.0, 17: 1.0, 18: 0.0, 19: 1.0, 20: 1.0, 21: 0.0, 22: 0.0, 23: 0.0, 24: 0.0, 25: 1.0, 26: 0.0, 27: 0.0, 28: 0.0, 29: 1.0, 30: 0.0, 31: 1.0, 32: 1.0, 33: 1.0, 34: 1.0, 35: 0.0, 36: 0.0, 37: 0.0, 38: 0.0, 39: 1.0, 40: 1.0, 41: 0.0, 42: 0.0, 43: 1.0, 44: 0.0, 45: 1.0, 46: 0.0, 47: 0.0, 48: 1.0, 49: 1.0}, 'Var2': {0: 0.0, 1: 0.0, 2: 1.0, 3: 1.0, 4: 0.0, 5: 1.0, 6: 0.0, 7: 0.0, 8: 0.0, 9: 1.0, 10: 1.0, 11: 0.0, 12: 1.0, 13: 0.0, 14: 1.0, 15: 0.0, 16: 1.0, 17: 0.0, 18: 0.0, 19: 1.0, 20: 0.0, 21: 1.0, 22: 1.0, 23: 0.0, 24: 1.0, 25: 0.0, 26: 1.0, 27: 0.0, 28: 1.0, 29: 0.0, 30: 0.0, 31: 1.0, 32: 1.0, 33: 0.0, 34: 1.0, 35: 0.0, 36: 1.0, 37: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 41: 0.0, 42: 1.0, 43: 1.0, 44: 0.0, 45: 0.0, 46: 1.0, 47: 0.0, 48: 1.0, 49: 1.0}, 'Var3': {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 1.0, 5: 1.0, 6: 0.0, 7: 1.0, 8: 1.0, 9: 1.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 1.0, 14: 0.0, 15: 0.0, 16: 1.0, 17: 0.0, 18: 1.0, 19: 1.0, 20: 0.0, 21: 1.0, 22: 0.0, 23: 1.0, 24: 0.0, 25: 1.0, 26: 1.0, 27: 1.0, 28: 1.0, 29: 0.0, 30: 1.0, 31: 0.0, 32: 1.0, 33: 1.0, 34: 1.0, 35: 0.0, 36: 1.0, 37: 1.0, 38: 0.0, 39: 0.0, 40: 0.0, 41: 1.0, 42: 1.0, 43: 1.0, 44: 1.0, 45: 0.0, 46: 1.0, 47: 1.0, 48: 0.0, 49: 1.0}, 'Var4': {0: 0.0, 1: 1.0, 2: 1.0, 3: 1.0, 4: 1.0, 5: 1.0, 6: 1.0, 7: 1.0, 8: 1.0, 9: 1.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.0, 15: 1.0, 16: 1.0, 17: 1.0, 18: 0.0, 19: 1.0, 20: 1.0, 21: 1.0, 22: 1.0, 23: 1.0, 24: 1.0, 25: 1.0, 26: 1.0, 27: 1.0, 28: 1.0, 29: 0.0, 30: 1.0, 31: 0.0, 32: 0.0, 33: 0.0, 34: 0.0, 35: 0.0, 36: 0.0, 37: 0.0, 38: 1.0, 39: 0.0, 40: 0.0, 41: 1.0, 42: 0.0, 43: 0.0, 44: 0.0, 45: 1.0, 46: 1.0, 47: 1.0, 48: 0.0, 49: 1.0}, 'Target': {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 1, 6: 0, 7: 0, 8: 1, 9: 1, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 1, 17: 0, 18: 0, 19: 1, 20: 0, 21: 1, 22: 0, 23: 0, 24: 0, 25: 1, 26: 1, 27: 0, 28: 1, 29: 0, 30: 0, 31: 0, 32: 1, 33: 0, 34: 1, 35: 0, 36: 0, 37: 0, 38: 0, 39: 0, 40: 0, 41: 0, 42: 0, 43: 1, 44: 0, 45: 0, 46: 1, 47: 0, 48: 0, 49: 1}}\n",
    "ans = random_forest(new_point, data)\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2b86c0",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\n",
    "- [A Step By Step Implementation of Principal Component Analysis](https://towardsdatascience.com/a-step-by-step-implementation-of-principal-component-analysis-5520cc6cd598)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e075c390",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mrequest to http://localhost:8888/api/sessions?1674748755844 failed, reason: connect ETIMEDOUT 127.0.0.1:8888. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62919c69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('py310')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4 (main, Mar 31 2022, 03:38:35) [Clang 12.0.0 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "d1a940d2a5a085e4f840c6fa90dace1e4a81d9a7fba180f0fcbb4947149ff9d2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
