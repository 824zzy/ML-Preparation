{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b3899b19",
   "metadata": {},
   "source": [
    "# Loss functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b09eefdd",
   "metadata": {},
   "source": [
    "-----\n",
    "### Dice Loss\n",
    "\n",
    "- Dice loss: $1 - \\frac{2|X \\cap Y|}{|X| + |Y|}$\n",
    "- The Dice coefficient, or Dice-SÃ¸rensen coefficient, is a common metric for pixel segmentation that can also be modified to act as a loss function.\n",
    "- Used in segmentation problems.\n",
    "\n",
    "### Cross Entropy Loss\n",
    "\n",
    "- Cross Entropy Loss: $- \\sum_{i=1}^{n}y_i \\log(\\hat{y_i})$.\n",
    "- For binary classification: $-y \\log(\\hat{y}) - (1 - y) \\log(1 - \\hat{y})$.\n",
    "- Cross-entropy loss (also known as log loss) is a common loss function used in supervised machine learning, particularly in classification problems. The function measures the dissimilarity between the predicted probability distribution and the true distribution.\n",
    "- Used in classification problems.\n",
    "\n",
    "### MAE Loss (L1 Loss)\n",
    "\n",
    "- MAE Loss: $\\frac{1}{n}\\sum_{i=1}^{n}|y_i - \\hat{y_i}|$.\n",
    "- Mean Absolute Error is the sum of absolute distances between our target variable and predicted values.\n",
    "- Used in regression problems.\n",
    "\n",
    "### MSE Loss (L2 Loss)\n",
    "\n",
    "- MSE Loss: $\\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y_i})^2$.\n",
    "- Mean Squared Error as the name suggests is the sum of squared distances between our target variable and predicted values.\n",
    "- Used in regression problems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042b37ee",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Regularization\n",
    "\n",
    "L1 and L2 regularization are techniques that discourage learning a more complex or flexible model, so as to **avoid the risk of overfitting**.\n",
    "\n",
    "Lasso and Ridge regression **add a penalty term to the linear regression loss function** to prevent overfitting. \n",
    "\n",
    "1. L1 Regularization (Lasso):\n",
    "   - L1 regularization adds a penalty term to the loss function that is proportional to the absolute values of the model's weights.\n",
    "   - The L1 regularization term is calculated as the sum of the absolute values of the weights: $\\lambda \\sum_{i=1}^{n}|w_i|$, where $\\lambda$ is the regularization strength.\n",
    "   - L1 regularization can be used to perform feature selection, as it encourages the weights of irrelevant features to be set to zero.\n",
    "   - L1 regularization can be useful when dealing with high-dimensional datasets with many irrelevant or redundant features, as it helps in automatic feature selection by eliminating less important features.\n",
    "2. L2 Regularization (Ridge):\n",
    "   - L2 regularization adds a penalty term to the loss function that is proportional to the square of the model's weights.\n",
    "   - The L2 regularization term is calculated as the sum of the squared values of the weights: $\\lambda \\sum_{i=1}^{n}w_i^2$, where $\\lambda$ is the regularization strength.\n",
    "   - L2 regularization is useful for reducing the impact of correlated features, as it encourages the weights of correlated features to be similar.\n",
    "   - L2 regularization can help in preventing overfitting by penalizing large weights and making the model more robust to noise in the training data.\n",
    "\n",
    "\n",
    "What's the difference between Lasso and Ridge regression?\n",
    "\n",
    "1. Use L1 regularization (Lasso) when:\n",
    "   - Feature selection is important, and you want to automatically eliminate less important features.\n",
    "   - You have high-dimensional data with many irrelevant or redundant features.\n",
    "   - You are interested in a sparse model with fewer non-zero coefficients.\n",
    "2. Use L2 regularization (Ridge) when:\n",
    "   - You want to prevent overfitting and improve generalization performance.\n",
    "   - You don't necessarily need feature selection or sparsity.\n",
    "   - You want a smoother optimization landscape that allows for faster convergence.\n",
    "\n",
    "When there are highly correlated features in your dataset, how would the weights for L1 and L2 end up being?\n",
    "1. L1 regularization tends to arbitrarily select one of the correlated features and set its weight to zero while keeping the others non-zero. \n",
    "2. L2 regularization tends to shrink the weights of highly correlated features towards each other, effectively reducing their magnitudes.\n",
    "\n",
    "In PyTorch, you can add regularization terms to the loss function. For L2 regularization (weight decay), you can use the weight_decay parameter in the optimizer. For example:\n",
    "\n",
    "`optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-5)`\n",
    "\n",
    "While L1 and L2 regularization are conceptually applied to the loss function, implementing them within optimizers in frameworks like PyTorch offers practical benefits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368ed21a",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Reinforcement Learning from Human Feedback (RLHF)\n",
    "\n",
    "Reinforcement Learning from Human Feedback (RLHF) is a framework for training reinforcement learning agents using human feedback. In RLHF, the agent interacts with the environment and receives feedback from a human teacher, which is used to update the agent's policy. The feedback can take various forms, such as binary rewards, preference comparisons, or natural language instructions. RLHF is designed to enable efficient and effective learning from human feedback, and it has applications in areas such as interactive machine learning, human-robot interaction, and personalized recommendation systems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac60c8c8",
   "metadata": {},
   "source": [
    "-----\n",
    "## Direct Preference Optimization (DPO)\n",
    "\n",
    "Direct Preference Optimization (DPO) is a technique used to optimize a model's parameters directly based on the preferences of the user. In DPO, the model's parameters are updated based on the user's preferences, rather than using a loss function to measure the model's performance.\n",
    "\n",
    "$$\\theta_{t+1} = \\theta_t + \\alpha \\nabla_{\\theta} J(\\theta)$$\n",
    "\n",
    "where $\\theta$ is the model's parameters, $\\alpha$ is the learning rate, and $\\nabla_{\\theta} J(\\theta)$ is the gradient of the objective function $J(\\theta)$ with respect to the model's parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a060f039",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- [https://stats.stackexchange.com/questions/126238/what-are-the-advantages-of-relu-over-sigmoid-function-in-deep-neural-networks](https://stats.stackexchange.com/questions/126238/what-are-the-advantages-of-relu-over-sigmoid-function-in-deep-neural-networks)\n",
    "- [https://towardsdatascience.com/fantastic-activation-functions-and-when-to-use-them-481fe2bb2bde](https://towardsdatascience.com/fantastic-activation-functions-and-when-to-use-them-481fe2bb2bde)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89993a60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
