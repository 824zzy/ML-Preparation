{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer\n",
    "\n",
    "-----\n",
    "### Gradient Descent VS Stochastic Gradient Descent\n",
    "\n",
    "**Gradient Descent** is to iteratively adjust the model parameters in the direction that reduces the cost function. It updates parameters with a fixed learning rate in each iteration, which can lead to slow convergence or overshooting.\n",
    "\n",
    "The algorithm is as follows:\n",
    "1. Initialize the model parameters with random values.\n",
    "2. Calculate the gradient of the cost function with respect to each parameter.\n",
    "3. Update the parameters in the opposite direction of the gradient.\n",
    "4. Repeat steps 2 and 3 until the cost function converges to a minimum.\n",
    "\n",
    "**Stochastic Gradient Descent (SGD)** is a variant of the gradient descent algorithm that updates the model parameters using the gradient of the cost function with respect to a small subset of the training data, rather than the entire dataset. This makes the algorithm faster and more scalable, especially for large datasets. However, SGD can have high variance in parameter updates, leading to oscillations.\n",
    "\n",
    "The algorithm is as follows:\n",
    "1. Initialize the model parameters with random values.\n",
    "2. Randomly shuffle the training data.\n",
    "3. For each mini-batch of the training data, calculate the gradient of the cost function with respect to each parameter.\n",
    "4. Update the parameters in the opposite direction of the gradient.\n",
    "5. Repeat steps 2, 3, and 4 until the cost function converges to a minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "### RMSprop (Root Mean Square Propagation) VS Adam (Adaptive Moment Estimation):\n",
    "1. RMSprop: \n",
    "- RMSprop is an adaptive learning rate optimization algorithm.\n",
    "- It maintains a moving average of squared gradients for each parameter.\n",
    "- RMSprop scales the learning rate inversely with the square root of the accumulated squared gradients, thus reducing the learning rate for parameters with frequently occurring large gradients.\n",
    "- This helps in dealing with the exploding gradients problem and speeds up convergence in some cases.\n",
    "- However, RMSprop does not adapt the learning rates individually for each parameter.\n",
    "\n",
    "2. Adam:\n",
    "- Adam is an extension of RMSprop that also incorporates momentum.\n",
    "- It maintains both a moving average of past gradients and a moving average of past squared gradients for each parameter.\n",
    "- Adam adapts the learning rates for each parameter individually by computing the effective learning rate for each parameter using the first and second moments of the gradients.\n",
    "- It combines the advantages of both RMSprop (adaptive learning rates) and momentum (accelerating convergence in relevant directions).\n",
    "- Adam is widely used in practice due to its efficiency and effectiveness in optimizing various types of neural networks.\n",
    "\n",
    "3. Differences between Adam and RMSprop:\n",
    "\n",
    "- Adam includes momentum, which helps accelerate convergence in relevant directions, while RMSprop does not have explicit momentum.\n",
    "- Adam adapts the learning rates for each parameter individually, while RMSprop uses the same learning rate for all parameters.\n",
    "- Adam typically requires more memory and computation than RMSprop due to the additional computations involved in maintaining momentum. However, Adam often converges faster and is more robust to hyperparameter choices in practice."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
