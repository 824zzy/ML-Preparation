{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b29b8a2",
   "metadata": {},
   "source": [
    "# Data Preprocessing Techniques for Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc09797",
   "metadata": {},
   "source": [
    "## 1. Bag of Words\n",
    "\n",
    "The idea is to **represent each sentence as a bag of words**, disregarding grammar and paradigms, i.e., just the occurrence of words in a sentence defines the meaning of the sentence for the model.\n",
    "\n",
    "For example: Given two sentence \"I have a dog\" and \"You have a cat\", the first sentence (“I have a dog”) representation becomes 1,1,1,1,0,0, while the second sentence (“You have a cat”) representation becomes 0,1,1,0,1,1.\n",
    "\n",
    "Pros and Cons of Bag of Words:\n",
    "\n",
    "1. Pros: \n",
    "    1. Simple to implement, easy to understand.\n",
    "2. Cons:\n",
    "    1. If our input data is big, that would mean that the vocabulary size will also increase. This, in turn, makes our representation matrix much larger and makes computations very complex.\n",
    "    2. Computational nightmare is the inclusion of many 0s in our matrix (i.e., a sparse matrix). A sparse matrix contains less information and wastes a lot of memory.\n",
    "    3. The biggest disadvantage in Bag-of-Words is the complete inability to learn grammar and semantics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c13e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bag of words\n",
    "def calculate_bag_of_words(text, sentence):\n",
    "    # create a dictionary for frequency check\n",
    "    freqDict = dict.fromkeys(text, 0)\n",
    "    # loop over the words in sentences\n",
    "    for word in sentence:\n",
    "        # update word frequency\n",
    "        freqDict[word]=sentence.count(word)\n",
    "    # return dictionary \n",
    "    return freqDict\n",
    "\n",
    "text = ['I', 'have', 'a', 'dog', 'you', 'cat']\n",
    "s1 = \"I have a dog\"\n",
    "s2 = \"You have a cat\"\n",
    "ans1 = calculate_bag_of_words(text, s1)\n",
    "ans2 = calculate_bag_of_words(text, s2)\n",
    "print(ans1)\n",
    "print(ans2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "41381a93",
   "metadata": {},
   "source": [
    "## 2. Word2Vec\n",
    "Word2Vec essentially means expressing each word in your text corpus in an N-dimensional space (embedding space).  We help define the meaning of words based on their context.\n",
    "\n",
    "There are two subsets of Word2Vec:\n",
    "1. Continuous Bag-of-Words (CBOW)\n",
    "2. SkipGram\n",
    "\n",
    "### 2.1 CBOW\n",
    "CBOW is a technique where, given the neighboring words, the center word is determined. If our input sentence is **“I am reading the book.”**, then the input pairs and labels for a window size of 3 would be:\n",
    "- I, reading, for the label am\n",
    "- am, the, for the label reading\n",
    "- reading, book, for the label \n",
    "\n",
    "### 2.2 Skip-Gram\n",
    "Skip-Gram approach is given the center word, we have to predict its neighboring words. Quite literally the opposite of CBOW, but more efficient. Before we get to that, let’s understand what Skip-Gram is.\n",
    "\n",
    "Let our given input sentence be “I am reading the book.” The corresponding Skip-Gram pairs for a window size of 3 would be:\n",
    "\n",
    "- am, for labels I and reading\n",
    "- reading, for labels am and the\n",
    "- the, for labels reading and \n",
    "\n",
    "### 2.3 CBOW vs Skip-Gram\n",
    "\n",
    "1. CBOW is faster than Skip-Gram because CBOW will predict the center word from the context words, while Skip-Gram will predict the context words from the center word.\n",
    "2. Skip-Gram is better for infrequent words or small dataset.\n",
    "\n",
    "### 2.4 Negative Sampling\n",
    "\n",
    "1. Negative Sampling is a technique to speed up the training of Word2Vec.\n",
    "2. The idea is to use only a small subset of negative samples to train the model. This is done by randomly sampling a small subset of negative samples based on the frequency of the words in the corpus.\n",
    "\n",
    "### 2.5 Drawbacks of Word2Vec\n",
    "\n",
    "1. Word2Vec does not capture the meaning of the word in the sentence. For example, the word “bank” can be used as a verb or a noun. Word2Vec will not be able to differentiate between the two.\n",
    "2. Word2Vec does not consider the order of the words in the sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59532a40",
   "metadata": {},
   "source": [
    "## 3. GLOVE\n",
    "\n",
    "Glove is based on **matrix factorization techniques** on the word-context matrix. It first constructs a large matrix of (words x context) co-occurrence information, i.e. for each “word” (the rows), you count how frequently we see this word in some “context” (the columns) in a large corpus.  The number of “contexts” is of course large, since it is essentially combinatorial in size.\n",
    "\n",
    "What is the different between Word2Vec and GLOVE?\n",
    "\n",
    "    Word2vec embeddings are based on training a shallow feedforward neural network while glove embeddings are learnt based on matrix factorization techniques."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aa23d19b",
   "metadata": {},
   "source": [
    "## 4. Transformer Tokenizer\n",
    "\n",
    "## Transformer Tokenizer Types\n",
    "\n",
    "1. Byte-Pair Encoding (BPE) <== (GPT-2, GPT-3): A subword-based tokenization algorithm, it iteratively replaces the most frequent pair of bytes in a sequence with a single, unused byte to produce a set of tokens that can better represent the original text. \n",
    "   1. Example: \"aaabdaaabac\" ==> \"ZabdZabac\", where Z = aa ==> \"ZYdZYac\", where Z = aa and Y = ab ==> \"XdXac\" where X = ZY, Y = ab, and Z = aa\n",
    "2. WordPiece Tokenizer <== (BERT, DistilBERT, and Electra): WordPiece is a subword tokenizer that is based on the intuition that words are composed of subwords. It is a very popular tokenizer that is used in NLP tasks. It is based on the idea that words are composed of subwords. For example, the word “bank” can be broken down into “ban” and “k”. The tokenizer first creates a vocabulary of all the characters in the corpus. Then, it iterates through the corpus and merges the most frequent pair of characters. It continues this process until the vocabulary size is equal to the desired size. The vocabulary size is a hyperparameter that can be tuned.\n",
    "3. SentencePiece Tokenizer <==(ALBERT, XLNet, Marian, and T5.): SentencePiece is a subword tokenizer that is based on the intuition that words are composed of subwords. It is a very popular tokenizer that is used in NLP tasks. It is based on the idea that words are composed of subwords. For example, the word “bank” can be broken down into “ban” and “k”. The tokenizer first creates a vocabulary of all the characters in the corpus. Then, it iterates through the corpus and merges the most frequent pair of characters. It continues this process until the vocabulary size is equal to the desired size. The vocabulary size is a hyperparameter that can be tuned.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f89cd1",
   "metadata": {},
   "source": [
    "## Reference\n",
    "- [Word2Vec: A Study of Embeddings in NLP](https://pyimagesearch.com/2022/07/11/word2vec-a-study-of-embeddings-in-nlp/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
