{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20815588",
   "metadata": {},
   "source": [
    "# Math in Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa26822c",
   "metadata": {},
   "source": [
    "## 1. Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071d6ed7",
   "metadata": {},
   "source": [
    "### 1.1 The trade-off between bias and variance\n",
    "\n",
    "Bias: how well a model fits data. \n",
    "    \n",
    "    Model with high bias pays very little attention to the training data and oversimplifies the model. It always leads to **high error** on training and test data.\n",
    "\n",
    "Variance: how much a model changes based on inputs\n",
    "\n",
    "    Model with high variance pays a lot of attention to training data and **does not generalize** on the data which it hasn’t seen before.\n",
    "\n",
    "- Low variance and high bias ==> underfitting (model too simple)\n",
    "- High variance and low bias ==> overfitting (model too complex)\n",
    "- High variance and high bias ==> garbage\n",
    "- Low variance and low bias ==> perfect"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fb589bbc",
   "metadata": {},
   "source": [
    "### 1.2 Differentiate between correlation and covariance\n",
    "\n",
    "**Covariance shows you how the two variables differ, whereas correlation shows you how the two variables are related.**\n",
    "\n",
    "Covariance is a statistical term that refers to a systematic relationship between two random variables in which a change in the other reflects a change in one variable.\n",
    "\n",
    "$$cov(X,Y) = \\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})}{n-1}$$\n",
    "\n",
    "Correlation is a measure that determines the degree to which two or more random variables move in sequence.\n",
    "\n",
    "$$corr(X,Y) = \\frac{cov(X,Y)}{\\sigma_X \\sigma_Y}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abb3cc1",
   "metadata": {},
   "source": [
    "### 1.3 Gradient Descent\n",
    "\n",
    "Gradient descent is an optimization algorithm used to find the values of parameters (coefficients) of a function (f) that minimizes a cost function (cost).\n",
    "Gradient descent is best used when the parameters cannot be calculated analytically (e.g. using linear algebra) and must be searched for by an optimization algorithm."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d28e2fda",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "### How to deal with overfitting\n",
    "1. Regularization. It involves a cost term for the features involved with the objective function\n",
    "2. Making a simple model. With lesser variables and parameters, the variance can be reduced \n",
    "3. Cross-validation methods like k-folds can also be used\n",
    "4. If some model parameters are likely to cause overfitting, techniques for regularization like LASSO can be used that penalize these parameters\n",
    "\n",
    "### Combat the curse of dimensionality\n",
    "\n",
    "1. Manual Feature Selection\n",
    "2. Principal Component Analysis (PCA)\n",
    "3. Multidimensional Scaling\n",
    "4. Locally linear embedding\n",
    "\n",
    "### Regularization\n",
    "\n",
    "A technique that discourages learning a more complex or flexible model, so as to avoid the risk of overfitting.\n",
    "\n",
    "Lasso and Ridge regression are both types of regularized linear regression, which means they **add a penalty term to the linear regression objective function** to prevent overfitting. The main difference between the two is the type of penalty term they use.\n",
    "\n",
    "- L1 norm: **Lasso (Least Absolute Shrinkage and Selection Operator) regression** adds a penalty term to the objective function that is proportional to the absolute value of the coefficients. This results in some of the coefficients becoming exactly zero, which effectively eliminates those features from the model. This is called feature selection and it helps in reducing the number of features in the model. Lasso regression is useful for models with many features and it can help to select a subset of those features that are most useful for predicting the target variable.\n",
    "\n",
    "$$Objective function = Sum of squared errors (SSE) + λ * Sum of absolute values of the coefficients (|w|)$$\n",
    "\n",
    "- L2 norm: **Ridge Regression**, on the other hand, adds a penalty term to the objective function that is proportional to the square of the coefficients. This results in all coefficients shrinking towards zero, but none of them becoming exactly zero. This helps to reduce the impact of any one feature on the model, which can help to prevent overfitting. Ridge Regression is useful when there are high multicollinearity between the features.\n",
    "\n",
    "Objective function = Sum of squared errors (SSE) + λ * Sum of squared values of the coefficients (w^2)\n",
    "\n",
    "In short, Lasso regression is useful for feature selection, while Ridge regression is useful for reducing the impact of correlated features.\n",
    "\n",
    "### How to deal with unbalanced dataset\n",
    "\n",
    "1. Oversampling or undersampling. Instead of sampling with a uniform distribution from the training dataset, we can use other distributions so the model sees a more balanced dataset.\n",
    "2. **Data augmentation**. A\n",
    "   1. Add data in the less frequent categories by modifying existing data in a controlled way: Flip the images with illnesses; Add noise to copies of the images in such a way that the illness remains visible.\n",
    "3. **Using appropriate metrics**. In asteroid is going to hit the earth example, if we had a model that always made negative predictions, it would achieve a precision of 99.9%. There are other metrics such as precision, recall, and F-score that describe the accuracy of the model better when using an imbalanced dataset.\n",
    "\n",
    "### What is Momentum (w.r.t NN optimization)?\n",
    "\n",
    "Momentum lets the optimization algorithm remembers its last step, and adds some proportion of it to the current step. This way, even if the algorithm is stuck in a flat region, or a small local minimum, it can get out and continue towards the true minimum.\n",
    "\n",
    "### What is the difference between Batch Gradient Descent and Stochastic Gradient Descent?\n",
    "\n",
    "Batch gradient descent computes the gradient using the whole dataset. This is great for convex, or relatively smooth error manifolds. In this case, we move somewhat directly towards an optimum solution, either local or global. Additionally, batch gradient descent, given an annealed learning rate, will eventually find the minimum located in it's basin of attraction.\n",
    "\n",
    "Stochastic gradient descent (SGD) computes the gradient using a single sample. SGD works well (Not well, I suppose, but better than batch gradient descent) for error manifolds that have lots of local maxima/minima. In this case, the somewhat noisier gradient calculated using the reduced number of samples tends to jerk the model out of local minima into a region that hopefully is more optimal.\n",
    "\n",
    "### What is vanishing gradient?\n",
    "\n",
    "As we add more and more hidden layers, back propagation becomes less and less useful in passing information to the lower layers. In effect, as information is passed back, the gradients begin to vanish and become small relative to the weights of the networks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52680948",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
