{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a8dc457",
   "metadata": {},
   "source": [
    "# Layers in Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb46535",
   "metadata": {},
   "source": [
    "## Dropout\n",
    "\n",
    "During training, randomly zeroes some of the elements of the input tensor with probability $p$ using samples from a Bernoulli distribution. Each channel will be zeroed out independently on every forward call."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e2c2bb",
   "metadata": {},
   "source": [
    "## Batch Normalization\n",
    "\n",
    "Batch Normalization formula: $y = \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} * \\gamma + \\beta$\n",
    "\n",
    "where $\\mu$ is the mean of the input, $\\sigma^2$ is the variance of the input, $\\epsilon$ is a small value to avoid dividing by zero, $\\gamma$ is a learnable parameter, and $\\beta$ is a learnable parameter.\n",
    "\n",
    "Batch Normalization makes sure that the values of hidden units have standardized mean and variance. The BatchNorm layer is usually added before ReLU as mentioned in the Batch Normalization paper.\n",
    "\n",
    "Advantages of Batch Normalization:\n",
    "\n",
    "1. **Allow larger learning rates**: larger learning rates can cause vanishing/exploding gradients. However, since batch normalization takes care of that, larger learning rates can be used without worry.\n",
    "2. **Reduces overfitting**: Batch normalization has a regularizing effect since it adds noise to the inputs of every layer. This discourages overfitting since the model no longer produces deterministic values for a given training example alone."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8d0f6eda",
   "metadata": {},
   "source": [
    "## MultiheadAttention\n",
    "\n",
    "Multi-Head Attention consists of several attention layers running in parallel as shown in the figure below. Each attention layer has a different set of learnable parameters. The outputs of the different attention layers are concatenated and then put through a final linear layer.\n",
    "\n",
    "![mha](images/2022-09-08-11-01-25.png)\n",
    "\n",
    "Time complexity: O(N^2*d), where N is the sequence length and d is the representation dimension.\n",
    "\n",
    "### Positional Encoding\n",
    "\n",
    "Positional encoding describes the location or position of an entity in a sequence so that each position is assigned a unique representation. There are many reasons why a single number, such as the index value, is not used to represent an itemâ€™s position in transformer models. \n",
    "\n",
    "For long sequences, the indices can grow large in magnitude. If you normalize the index value to lie between 0 and 1, it can create problems for variable length sequences as they would be normalized differently.\n",
    "\n",
    "$$PE(i, 2i) = sin(i/10000^{(2i/d)})$$\n",
    "$$PE(i, 2i+1) = cos(i/10000^{(2i/d)})$$\n",
    "\n",
    "The implementation of positional encoding is as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3069fffd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def get_positional_encoding(max_len, d_model):\n",
    "    \"\"\"\n",
    "    Returns positional encoding for a given maximum length and embedding dimension.\n",
    "    \"\"\"\n",
    "    pos_encoding = torch.zeros(max_len, d_model)\n",
    "    for pos in range(max_len):\n",
    "        for i in range(0, d_model, 2):\n",
    "            pos_encoding[pos, i] = torch.sin(torch.tensor(pos / (10000 ** ((2 * i) / d_model))))\n",
    "            pos_encoding[pos, i + 1] = torch.cos(torch.tensor(pos / (10000 ** ((2 * (i + 1)) / d_model))))\n",
    "    return pos_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065ab0aa",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
