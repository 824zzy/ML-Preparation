{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a8dc457",
   "metadata": {},
   "source": [
    "# Layers in Deep Learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1bb46535",
   "metadata": {},
   "source": [
    "## Dropout\n",
    "\n",
    "During training, randomly zeroes some of the elements of the input tensor with probability $p$ using samples from a Bernoulli distribution. Each channel will be zeroed out independently on every forward call.\n",
    "\n",
    "### Normal Dropout VS Inverted Dropout\n",
    "- With normal dropout at test time you have to scale activations by dropout rate p because you are not dropping out any of the neurons, so you need to match expected value at training.\n",
    "- With inverted dropout, scaling is applied at the training time, but inversely. First, dropout all activations by dropout factor p, and second, scale them by inverse dropout factor 1/p.\n",
    "- Inverted dropout has an advantage, that you don’t have to do anything at test time, which makes inference faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5da85db",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mrequest to http://localhost:8888/api/sessions?1675408640126 failed, reason: connect ETIMEDOUT 127.0.0.1:8888. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# dropout layer\n",
    "dropout = torch.nn.Dropout(p=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e2c2bb",
   "metadata": {},
   "source": [
    "## Batch Normalization\n",
    "\n",
    "Batch Normalization formula: $y = \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} * \\gamma + \\beta$\n",
    "\n",
    "where $\\mu$ is the mean of the input, $\\sigma^2$ is the variance of the input, $\\epsilon$ is a small value to avoid dividing by zero, $\\gamma$ is a learnable parameter, and $\\beta$ is a learnable parameter.\n",
    "\n",
    "Batch Normalization makes sure that the values of hidden units have standardized mean and variance. The BatchNorm layer is usually added before ReLU as mentioned in the Batch Normalization paper.\n",
    "\n",
    "Advantages of Batch Normalization:\n",
    "\n",
    "1. **Allow larger learning rates**: larger learning rates can cause vanishing/exploding gradients. However, since batch normalization takes care of that, larger learning rates can be used without worry.\n",
    "2. **Reduces overfitting**: Batch normalization has a regularizing effect since it adds noise to the inputs of every layer. This discourages overfitting since the model no longer produces deterministic values for a given training example alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a0cb6c",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mrequest to http://localhost:8888/api/sessions?1675408640126 failed, reason: connect ETIMEDOUT 127.0.0.1:8888. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# batch normalization layer\n",
    "batch_norm = torch.nn.BatchNorm1d(num_features=100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8d0f6eda",
   "metadata": {},
   "source": [
    "## MultiheadAttention\n",
    "\n",
    "Multi-Head Attention consists of several attention layers running in parallel as shown in the figure below. Each attention layer has a different set of learnable parameters. The outputs of the different attention layers are concatenated and then put through a final linear layer.\n",
    "\n",
    "![mha](images/2022-09-08-11-01-25.png)\n",
    "\n",
    "Time complexity: O(N^2*d), where N is the sequence length and d is the representation dimension.\n",
    "\n",
    "### Positional Encoding\n",
    "\n",
    "Positional encoding describes the location or position of an entity in a sequence so that each position is assigned a unique representation. There are many reasons why a single number, such as the index value, is not used to represent an item’s position in transformer models. \n",
    "\n",
    "For long sequences, the indices can grow large in magnitude. If you normalize the index value to lie between 0 and 1, it can create problems for variable length sequences as they would be normalized differently.\n",
    "\n",
    "$$PE(i, 2i) = sin(i/10000^{(2i/d)})$$\n",
    "$$PE(i, 2i+1) = cos(i/10000^{(2i/d)})$$\n",
    "\n",
    "The implementation of positional encoding is as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3069fffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def get_positional_encoding(max_len, d_model):\n",
    "    \"\"\"\n",
    "    Returns positional encoding for a given maximum length and embedding dimension.\n",
    "    \"\"\"\n",
    "    pos_encoding = torch.zeros(max_len, d_model)\n",
    "    for pos in range(max_len):\n",
    "        for i in range(0, d_model, 2):\n",
    "            pos_encoding[pos, i] = torch.sin(torch.tensor(pos / (10000 ** ((2 * i) / d_model))))\n",
    "            pos_encoding[pos, i + 1] = torch.cos(torch.tensor(pos / (10000 ** ((2 * (i + 1)) / d_model))))\n",
    "    return pos_encoding"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4 (main, Mar 31 2022, 03:38:35) [Clang 12.0.0 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "d1a940d2a5a085e4f840c6fa90dace1e4a81d9a7fba180f0fcbb4947149ff9d2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
